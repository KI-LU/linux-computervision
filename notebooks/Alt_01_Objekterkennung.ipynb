{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d012b01f-abea-40d7-bcba-5918690e6b8d",
   "metadata": {},
   "source": [
    "# 1. Erstellen eines Datensatzes und Annotieren der Bilder\n",
    "\n",
    "Um einen Objekterkennungs-Algorithmus zu trainieren braucht es, wie für alle Deep Learning Anwendungen, Daten. Aus diesen Daten wird das Modell später versuchen, die Merkmale herauszuziehen, die für das Erkennen des jeweiligen Objektes relevant sind. \n",
    "\n",
    "![Image](../beispielbilder/grafiken/object-detection-example.PNG)\n",
    "**Objekterkennung mit Bounding-Boxen** *- das Modell erkennt verschiedene Objekte, wie Autos, Menschen, Hydranten, etc. und identifiziert sie im Bild, indem eine Box um das jeweilige Objekt gezogen wird. Die Zahl gibt jeweils an, wie sicher sich das Modell ist, dass es sich beim identifizierten Objekt tatsächlich um ein solches handelt.*\n",
    "\n",
    "Um das Verständnis zu vertiefen und um auf wichtige Aspekte aufmerksam zu machen haben wir eine Reihe von Fragen und Aufgaben in dieses Notebook eingebaut:\n",
    "\n",
    "**Beispiel**\n",
    "\n",
    "1. **Verständnisfragen**\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Frage:</b> In den blauen Boxen stehen Verständnisfragen</div>\n",
    "\n",
    "2. **Arbeitsaufträge**\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> In grünen Boxen finden sich Arbeitsaufträge.</div>\n",
    "\n",
    "Besonders komplizierte Aspekte oder Bereiche, in denen leicht Fehler gemacht werden können sind mit einer roten Box gekennzeichnet:\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Hier müssen Sie gut aufpassen. Nehmen Sie sich ausreichend Zeit, die nachfolgenden Angaben zu lesen oder Fragen Sie ein Mitglied des Teams um Hilfe.</div>\n",
    "\n",
    "**Was *ist* Objekterkennung?**\n",
    "\n",
    "Da es sich hierbei um ein Problem des **überwachten Lernens** handelt, reicht es nicht einfach, dem Algorithmus nur die Bilddaten zuzuführen. Damit eine Zuordnung von Bild &rarr; Objekt stattfinden kann, müssen die Daten entsprechend **annotiert** sein. \n",
    "\n",
    "<details>\n",
    "    <summary><b>Exkurs: Algorithmendesign vs. Datenkuration</b></summary>\n",
    "    <h5>Wo liegen heute die Constraints?</h5>\n",
    "\n",
    "In vielen Bereichen des Deep Learning spielt das Design der Algorithmen heute nicht mehr die Hauptrolle. Insbesondere in der maschinellen              Bildverarbeitung, also <b>Computer Vision</b> sind die <i>besten</i> (oder zumindest: sehr gute) Algorithmen heute bekannt. Der Constraint für die     Performance der Modelle liegt also heute weniger im Bereich der Architektur, sondern mehr in der Qualität der Datensätze. \n",
    "    \n",
    "    \n",
    "</details>\n",
    "<br>\n",
    "Das Annotieren von Daten kann teilweise automatisiert werden, wird aber in den meisten Fällen noch von Menschen per Hand durchgeführt. \n",
    "\n",
    "In diesem Notebook widmen wir uns also drei Dingen:\n",
    "\n",
    "1. **Dem Sammeln von Bilddaten**\n",
    "2. **Der Datenannotation**\n",
    "3. **Finetuning eines SOTA-Objekterkenners**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86483f4b-27af-47f4-9b6c-9848788b0261",
   "metadata": {},
   "source": [
    "### 1.1. Wo bekommt man die Daten her?\n",
    "\n",
    "Je nach Problemstellung kann es leichter oder schwieriger sein, an (gute) Daten heranzukommen. Für einige Domänen existieren bereits große - teilweise auch gut aufbereitete - Datensätze, die oft auch frei zugänglich im Internet abrufbar sind. \n",
    "\n",
    "Beispiele sind:\n",
    "\n",
    "* ImageNet\n",
    "* NIST-Datensätze\n",
    "* MS COCO\n",
    "* u.v.m.\n",
    "\n",
    "Die Klassen des **MS COCO** - Datensatzes. **COCO** steht dabei für **C**ommon **O**bjects in **CO**ntext.\n",
    "\n",
    "![Image](../beispielbilder/grafiken/coco.JPG)\n",
    "\n",
    "Hier einige Beispielbilder mit **Bounding-Boxen**:\n",
    "\n",
    "![Image](../beispielbilder/grafiken/coco_examples.JPG)\n",
    "\n",
    "<a href=\"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8839032&tag=1\">Quelle</a>\n",
    "\n",
    "Eine kleine Übersicht über populäre Datensätze findet sich <a href=\"https://datagen.tech/guides/image-datasets/image-datasets/#\">hier</a>.\n",
    "Es existieren auch viele weitere Repositories, über die Daten verhältnismäßg einfach bezogen werden können. \n",
    "\n",
    "Viele Problemstellungen sind jedoch sehr spezifisch für das Unternehmen und den konkreten Anwendungskontext. Hier ist es deutlich weniger wahrscheinlich, dass bereits gute Datensätze existieren oder dass diese - falls es sie gibt - frei verfügbar sind.\n",
    "\n",
    "In diesem Fall müssen die Daten erst selbst gesammelt und annotiert werden. Das kann - je nach Problem - einen **großen bis sehr großen Aufwand bedeuten**. \n",
    "\n",
    "Wir werden uns im Folgenden selbst einen Datensatz erstellen und diesen annotieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca778233-dbf6-4bd6-ab10-6ce69d49b023",
   "metadata": {},
   "source": [
    "## 1.2. Der Kontext\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Ihr Unternehmen stellt eine Reihe an Werkzeugen und anderen Bauteilen her. Einige dieser Objekte werden über das selbe Förderband weiter zum Lager transportiert. Aktuell müssen Menschen die Objekte per Hand in die korrekten Aufbewahrungsbehältnisse sortieren. \n",
    "\n",
    "Sie möchten diesen Prozess automatisieren und hatten die Idee, künstliche Intelligenz dafür zu verwenden.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3432317",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4582ea8f-8323-48df-8aed-b075a31afaab",
   "metadata": {},
   "source": [
    "## 1.3. Importieren der notwendigen Bibliotheken\n",
    "\n",
    "Um die Bilder aufzunehmen und diese zu rendern, verwenden wir die Computer-Vision Bibliothek <a href=\"https://opencv.org/\"><b>OpenCV</b></a>. Ein tieferes Verständnis der Bibliothek wird nicht vorausgesetzt.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus um die benötigten Bibliotheken zu importieren.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74f33bf4-01c8-45e1-94ce-fb49d504fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                                                         # OpenCV\n",
    "import os                                                          # Betriebssystem\n",
    "from ultralytics import YOLO                                       # Machine Learning Bibliothek\n",
    "from IPython.display import Video                                  # Zum Anzeigen von Videodateien\n",
    "\n",
    "# Die folgenden Funktionen dienen der Unterstützung beim Erstellen der Daten und der Verwendung des Modells\n",
    "###########################################################################################################\n",
    "from helpers.image_helpers import capture_images             # Helferfunktion zum Aufnehmen der Bilder\n",
    "from helpers.model_helpers import inference_webcam           # Helferfunktion zum Anwenden des Modells mit der Webcam\n",
    "from helpers.model_helpers import inference_video            # Helferfunktion zum Anwenden des Modells mit einer Videodatei\n",
    "from helpers.model_helpers import choose_model               # Helferfunktion zur Auswahl des Modells\n",
    "from helpers.model_helpers import set_training_config        # Helferfunktion zum Einstellen der Trainingskonfiguration\n",
    "from helpers.model_helpers import get_trained_model          # Helferfunktion zum Extrahieren eines trainierten Modells\n",
    "from helpers.file_handlers import prepare_folder_structure   # Helferfunktion zum Erstellen der benötigten Ordnerstruktur\n",
    "from helpers.file_handlers import create_config              # Helferfunktion zum Erstellen der Konfigurationsdatei\n",
    "from helpers.file_handlers import move_file                  # Helferfunktion zum Speichern des Modells\n",
    "from helpers.config import set_metadata                      # Helferfunktion zum Erstellen des Ordners\n",
    "from helpers.config import set_image_data                    # Helferfunktion zum Erstellen des Ordners\n",
    "from helpers.file_handlers import cleanup_images\n",
    "\n",
    "TASK = 'DETECT'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7975b213-5cb8-4ad5-982b-3eeaf0569578",
   "metadata": {},
   "source": [
    "## 1.4. Aufnehmen der Bilder\n",
    "\n",
    "Bevor wir anfangen, legen wir zwei übergeordnete Dinge fest:\n",
    "\n",
    "* Den Speicherort der Bilder.\n",
    "* Die Anzahl an Bildern, die wir sammeln wollen.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus und legen Sie einen Gruppennamen fest.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c4cc5d7-111c-4f89-9c54-ef7b2d5644f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gruppenordner erstellt: ../data/detection/fatehannika\n"
     ]
    }
   ],
   "source": [
    "GROUP, PATH = set_metadata(TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6884b-cad6-4f46-bb65-bcdfd8361af8",
   "metadata": {},
   "source": [
    "Anschließend werden die Bilder mithilfe der Webcam aufgenommen, mit einer ID versehen und dann abgespeichert. \n",
    "\n",
    "Setzen des `Device`-Parameters:\n",
    "\n",
    "<details>\n",
    "<summary>Ein Hinweis zu CV2</summary>\n",
    "    \n",
    "In der `capture_images`-Funktion bestimmt die Zeile\n",
    "    \n",
    "```Python\n",
    "cap = cv2.VideoCapture(0)\n",
    "```\n",
    "<br>\n",
    "welche Kamera zur Aufnahme verwendet wird. \n",
    "\n",
    "</details><br>\n",
    "\n",
    "Normalerweise ist die primäre Kamera das Gerät `0`. Wenn eine andere Kamera verwendet werden soll, muss dieser Wert auf `1` geändert werden.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus um Bilder aufzunehmen.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26212180-c0cc-4e02-af82-aa3b8598f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, NUM_IMGS, DELAY, DEVICE = set_image_data(device=0, task=TASK)\n",
    "\n",
    "capture_images(num_imgs=NUM_IMGS,\n",
    "               name=GROUP,\n",
    "               img_path=PATH,\n",
    "               device=DEVICE,\n",
    "               delay=DELAY,\n",
    "               show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051449-3b09-4001-9f46-2d89ec8b8e0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Auftrag:</b> Sie können die obere Codezelle beliebig oft ausführen um weitere Aufnahmen zu machen.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b82ac-7cf5-488c-b37f-e99e3d2f64dc",
   "metadata": {},
   "source": [
    "# 2. Annotieren mit Label-Studio\n",
    "\n",
    "Zum Annotieren der Bilder verwenden wir <a href=\"https://labelstud.io/\">Label-Studio</a>. Die Auswahl der Annotations-Software spielt in diesem Fall eine untergeordnete Rolle. Wir könnten genauso gut eine andere Software verwenden (z.b. <a href=\"https://www.cvat.ai/\">CVAT</a>). \n",
    "\n",
    "Sollten Sie bereits eine Annotationssoftware kennen, verwenden Sie am besten die, mit der Sie am vertrautesten sind. Wichtig ist lediglich, dass sich über die Software die Daten im `YOLO`-Format exportieren lassen. \n",
    "\n",
    "**Standardmäßig haben wir auf unseren Geräten Label Studio bereits vorinstalliert.**\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Der Label-Studio-Server sollte bereits gestartet sein und in einem anderen Fenster laufen. Sollte dies nicht der Fall sein, bitten Sie bitte kurz ein Teammitglied um Hilfe.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b4708-c2ab-47b5-9e82-45071275e851",
   "metadata": {},
   "source": [
    "### 2.1. Eine kurze Tour durch Label-Studio\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Machen Sie sich mit der Funktionsweise von Label Studio vertraut und annotieren Sie die Bilder. Dafür können Sie dem untenstehenden Guide folgen, der Ihnen eine kurze Einführung in das Programm gibt. Oder fragen Sie uns, wir erklären Ihnen gerne die Funktionsweise von Label-Studio.</div>\n",
    "\n",
    "**1. Home-Screen: Projektübersicht**\n",
    "\n",
    "   Nach dem Öffnen landen Sie in der Projektübersicht. \n",
    "\n",
    "   <div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Wählen Sie das Projekt <b>Lernumgebung_Gruppe_</b> aus.</div>\n",
    "   \n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide1.png)\n",
    "\n",
    "**2. Projektansicht**\n",
    "\n",
    "   Die Projektansicht ist noch leer. Das liegt daran, dass noch keine Daten in das Projekt geladen wurden.\n",
    "   Dies kann man über den **Import**-Button tun.\n",
    "\n",
    "   <div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Laden Sie die aufgenommenen Bilder in Label-Studio. Dazu wählen Sie <b>upload files</b> aus und navigieren dort zum Projektordner.</div>\n",
    "\n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide2.png)\n",
    "\n",
    "   Die Bilder erscheinen dann in der Projektübersicht.\n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide3.png)\n",
    "\n",
    "**3. Bestimmen der Label: Settings**\n",
    "\n",
    "   Bevor mit dem Annotieren begonnen werden kann, müssen noch die Label festgelegt werden. Dies kann über die **Settings** (oben rechts) tun.\n",
    "\n",
    "**4. Settings: Labeling Interface**\n",
    "\n",
    "   Navigieren Sie zum Punkt **Labeling Interface**.\n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide4.png)\n",
    "\n",
    "\n",
    "\n",
    "**5. Labeling Interface: **Browse templates****\n",
    "\n",
    "Wählen Sie **Object detection with bounding boxes**. Hier können Sie im Feld **Add label names** jetzt Ihre Klassen hinzufügen. Dafür geben Sie einfach für jede Klasse den Namen in das Feld und fügen diese mit **Add** den Labels hinzu. Sie können auch mehrere Klassen auf einmal hinzufügen. Dafür muss für jede Klasse eine eigene Zeile angefangen werden. Nach Bedarf kann auch die Farbe der Labels verändert werden. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Fügen Sie die gewünschten Label hinzu.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Nachdem alle Label hinzugefügt wurden, muss das mit <b>Save</b> bestätigt werden!</div>\n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide5.png)\n",
    "\n",
    "\n",
    "\n",
    "**6. Projektübersicht: Label All Tasks**\n",
    "\n",
    "   Navigieren Sie zurück in die Übersicht (Projects/Lernumgebung). Jetzt können Sie anfangen, die Bilder zu annotieren.\n",
    "\n",
    "   <div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Öffnen Sie die Annotier-Ansicht. Klicken Sie dafür auf <b>Label All Tasks</b></div>\n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide6.png)\n",
    "\n",
    "\n",
    "\n",
    "**7. Annotieren: Bounding Boxes**\n",
    "\n",
    "   Die Annotations-Ansicht besteht zum einen aus dem aktuellen Bild, den Labels / Klassen (unten) und zwei weiteren Bereichen (rechts). Letztere sind für den weiteren Verlauf nicht von Bedeutung.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Pro Bild können <b>beliebig</b> viele Objekte annotiert werden. Es sollten natürlich möglichst alle Vorkommnisse eines Objekts annotiert werden.!</div>\n",
    "   \n",
    "   Es gibt verschiedene Möglichkeiten, Bilder zu annotieren. In diesem Projekt verwenden wir die **Bounding Box**. Dabei handelt es sich um eine rechteckige Box, die über das jeweilige Objekt gezogen wird. Im Bild unten sind die Objekte beispielsweise Bilderrahmen, Pflanzen, Sofas und Tische.\n",
    "\n",
    "   ![Image](../beispielbilder/guides/labelstudio-guide/ls_guide7.png)\n",
    "\n",
    "Um ein Objekt einer Klasse zu annotieren **muss** vorher das entsprechende Label unten ausgewählt werden (hier kann auch der Shortcut (die entsprechende Zahl, die jeweils rechts im Label steht) verwendet werden). **Anschließend** wird mit der Maus eine **rechteckige Box** um das Objekt gezogen. Die Box sollte so eng wie möglich um das Objekt gezogen werden.\n",
    "\n",
    "   Wenn alle Objekte auf dem Bild annotiert wurden, können die Annotationen mit **Submit** gespeichert werden - das nächste Bild öffnet sich dann automatisch. Unerwünschte Bilder können mit **Skip** übersprungen werden.\n",
    "\n",
    "   Das wird so lange gemacht, bis alle Bilder annotiert wurden.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Annotieren Sie die Bilder in Ihrem Projekt.</div>\n",
    "\n",
    "\n",
    "**8. Projektübersicht: Exportieren**\n",
    "\n",
    "   Navigieren Sie nun zurück in die Übersicht und exportieren Sie die Bilder mit den Annotationen im **YOLO-Format**.\n",
    "\n",
    "   <div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Exportieren Sie die Bilder im YOLO-Format. Dazu wählen Sie zunächst <b>Export</b> aus und anschließend das <b>YOLO</b>-Format. Dann bestätigen Sie das ganze mit <b>Export</b>. Jetzt werden die Daten automatisch als ZIP-Datei heruntergeladen. </div>\n",
    "\n",
    "   \n",
    "\n",
    "![Image](../beispielbilder/guides/labelstudio-guide/ls_guide8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98763911-b4ee-4f08-bcae-2de1ac84fa3f",
   "metadata": {},
   "source": [
    "# 3. Vorbereiten der Bilddaten für das Modelltraining\n",
    "\n",
    "Bevor mit dem Modelltraining begonnen werden kann, müssen die annotierten Daten noch in die entsprechenden Ordner entzippt werden.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Entpacken Sie den eben heruntergeladenen ZIP-Ordner in den <code>data/detect/IHR_GRUPPENNAME</code>-Ordner.</div>\n",
    "\n",
    "Im Ordner befinden sich mehrere Dinge:\n",
    "\n",
    ">* images\n",
    ">* labels\n",
    ">* classes.txt\n",
    ">* notes.json\n",
    "\n",
    "Im `images`-Ordner befinden sich die Bilder und im `labels`-Ordner die dazugehörigen Labels. Das YOLO-Modell erwartet, dass die Bilder und Label nocheinmal aufgeteilt werden in ein **Trainingsset** und ein **Validationsset**. \n",
    "\n",
    "Sie brauchen also diese Ordnerstruktur:\n",
    "\n",
    "* images\n",
    "    * train\n",
    "    * val\n",
    "* labels\n",
    "    * train\n",
    "    * val\n",
    "* classes.txt\n",
    "* notes.json\n",
    "\n",
    "Die ```prepare_folder_structure(PATH)```-Funktion stellt diese Ordnerstruktur automatisch für Sie her und teilt die Bilder und Label in **Trainings-** und **Validationsdaten** ein.\n",
    "\n",
    "Anschließend wird über die `create_config(PATH)`-Funktion die Konfigurationsdatei erstellt, die dem Modell sagt, wo die Daten zu finden sind.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nachfolgende Codezelle mit einem Klick auf den Play-Button aus.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9db1f811-f5d9-4260-8329-6b6533b222c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprepare_folder_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTASK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m create_config(PATH, task=TASK)\n\u001b[32m      3\u001b[39m cleanup_images(PATH)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/modules/computervision/notebooks/helpers/file_handlers.py:182\u001b[39m, in \u001b[36mprepare_folder_structure\u001b[39m\u001b[34m(path, task, val_size, test_size)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_size == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    181\u001b[39m     val_size = \u001b[32m0.2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_detect_folder_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/modules/computervision/notebooks/helpers/file_handlers.py:84\u001b[39m, in \u001b[36mprepare_folder_structure.<locals>.prepare_detect_folder_structure\u001b[39m\u001b[34m(path, val_size)\u001b[39m\n\u001b[32m     81\u001b[39m         valid_label.append(instance[\u001b[32m1\u001b[39m])\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# creating train and val split\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m x_train, x_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_img\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_label\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# if not sorted, doesnt work on unix based systems occasionally\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# asserting equality --> files in train and val MUST match\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstrip_extension\u001b[39m(file_list, extension):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/modules/computervision/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/modules/computervision/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2851\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2848\u001b[39m arrays = indexable(*arrays)\n\u001b[32m   2850\u001b[39m n_samples = _num_samples(arrays[\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2851\u001b[39m n_train, n_test = \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2852\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\n\u001b[32m   2853\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m   2856\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/modules/computervision/.venv/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2481\u001b[39m, in \u001b[36m_validate_shuffle_split\u001b[39m\u001b[34m(n_samples, test_size, train_size, default_test_size)\u001b[39m\n\u001b[32m   2478\u001b[39m n_train, n_test = \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[32m   2480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2481\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2482\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2483\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2484\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maforementioned parameters.\u001b[39m\u001b[33m\"\u001b[39m.format(n_samples, test_size, train_size)\n\u001b[32m   2485\u001b[39m     )\n\u001b[32m   2487\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[31mValueError\u001b[39m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "prepare_folder_structure(PATH, task=TASK)\n",
    "create_config(PATH, task=TASK)\n",
    "cleanup_images(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ee944-ba91-438b-80f4-0466c4de0bf3",
   "metadata": {},
   "source": [
    "Ihr `IHR_GRUPPENNAME`-Order sollte nun so aussehen:\n",
    "\n",
    "![Image](../beispielbilder/grafiken/ordner-detect.PNG)\n",
    "\n",
    "Außerdem sollte sich eine `config.yaml` - Datei im Ordner befinden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb04c0-5b98-4010-b409-170228a95019",
   "metadata": {},
   "source": [
    "# 4. Auswahl eines geeigneten Modells und Fine-Tuning für eigene Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47103711-c455-4749-9c79-a31895e446b3",
   "metadata": {},
   "source": [
    "Eine der erfolgreichsten Objekterkennungs-Architekturen, ist die YOLO-Reihe. YOLO steht für:\n",
    "\n",
    "* **Y**ou\n",
    "* **O**nly\n",
    "* **L**ook\n",
    "* **O**nce\n",
    "\n",
    "Die YOLO-Modellarchitektur ermöglicht die schnelle und effiziente Erkennung von Objekten in Bildern und Videos. Im Gegensatz zu anderen Methoden benötigt YOLO nur einen Durchgang, um alle Objekte zu identifizieren. Es berücksichtigt unterschiedliche Objektgrößen und bietet Vielseitigkeit für verschiedene Anwendungen. YOLO zeichnet sich durch seine Geschwindigkeit und Fähigkeit aus, komplexe Szenen in einem Schritt zu erfassen.\n",
    "\n",
    "Wenn Sie mehr über die YOLO-Modelle lesen möchten, finden Sie <a href=\"https://blog.roboflow.com/guide-to-yolo-models/\">hier</a> einen guten Blog-Artikel und <a href=\"https://docs.ultralytics.com/\">hier</a> die Ultralytics-Dokumentation zum YOLOv8-Modell.\n",
    "\n",
    "<div class=\"alert block\" style=\"background-color: #D3D3D3;\">\n",
    "<b>Info:</b> Kürzlich wurde eine neue Version des Modells veröffentlicht: YOLOv10!</div>\n",
    "\n",
    "**Schematischer Aufbau** (keine Angst - wir werden in diesem Notebook nichts berechnen!)\n",
    "\n",
    "![Image](../beispielbilder/grafiken/yolo_functionality.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc07469-3262-42f3-a612-490804410d82",
   "metadata": {},
   "source": [
    "## 4.1. Auswahl des Modells\n",
    "\n",
    "Um eines der Modelle zu laden, erstellen muss einfach eine Instanz der `YOLO()`-Klasse erstellt werden. Dabei geben Sie die Modellarchitektur an, die Sie haben möchten.\n",
    "\n",
    "Es gibt verschiedene Varianten der YOLO-Modelle, die jeweils für andere **Tasks** gedacht sind (Objekterkennung, Klassifizierung, Bildsegmentierung, Posen/Punkterkennung)\n",
    "\n",
    "Für die Objekterkennung gibt es das Modell in verschiedenen Größen:\n",
    "\n",
    "![Image](../beispielbilder/grafiken/yolo.png)\n",
    "\n",
    "Je größer das Modell ist, desto besser - aber auch **teurer** (Rechenleistung) und **langsamer** (Rechenzeit) - wird das Modell.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Frage:</b> Überlegen Sie, in welchen Situationen welche Modellgröße verwendet werden sollte.</div>\n",
    "\n",
    "Die Syntax um ein Modell zu laden, lautet wie folgt:\n",
    "\n",
    "```Python\n",
    "\n",
    "modell = YOLO('modellversion')\n",
    "```\n",
    "\n",
    "Gibt man einen ganzen Pfad an und zuletzt die gewünscht Modellversion, wird das Modell automatisch an den Ort im Pfad heruntergeladen. Wir haben eine Helferfunktion geschrieben, die Ihnen den gewünschten Pfad zum Modell automatisch generiert.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus, um ein <code>YOLO</code>-Modell zu laden. Wählen Sie die Modellnumme raus, um das gewünschte Modell zu laden. Zum Beispiel, schreiben Sie 1, yolov8n (nano) zu verwenden.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05951611-4f5f-46ff-a93d-a962a9677f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_model(TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715dcc2a-abaf-4763-93f2-256d9262f9d6",
   "metadata": {},
   "source": [
    "## 4.2. Verwenden des Modells\n",
    "\n",
    "Nachdem das Modell geladen wurde, kann es direkt verwendet werden - **aber warum?** \n",
    "\n",
    "Die `.pt` - Datei (**P**y**T**orch) enthält ein **vortrainiertes Modell**. Dieses wurde auf dem COCO128 - Subset (**C**ommon **O**bjects in **CO**ntext) trainiert und ist in der Lage, eine große Anzahl von häufig vorkommenden Objekten zu erkennen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a716738-865a-45a9-9e07-8abf5512e381",
   "metadata": {},
   "source": [
    "### 4.2.1 Teste gerne das vortrainierte Modell mit der Webcam\n",
    "\n",
    "Das vortrainierte Modell kennt Ihre Bilder noch nicht. Testen Sie gerne das Modell aus und beobachten Sie, wie dieses auf Ihre Gegenstände reagiert.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus, um das vortrainierte Modell mit der Webcam zu verwenden.</div>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Achtung:</b> Sobald Sie den Code der nachsten Zelle ausführen aktiviert sich die eingebaute Webcam. Sie können dieses Fenster schließen indem Sie auf die Taste <b>q</b> drücken. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a92b2-d185-45de-bb4a-e375a6200c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_webcam(model=model,\n",
    "                 device=0,\n",
    "                 verbose=False) # Verbose bestimmt, ob der Modell-Output im Notebook angezeigt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c9442-b92a-4a60-935c-0041b356fb40",
   "metadata": {},
   "source": [
    "## 4.3. Finetuning des Modells auf den eigenen Bildern\n",
    "\n",
    "Nachdem Sie gesehen haben, wie man das Modell grundsätzlich verwenden kann, sollten Sie es nun auf Ihre eigenen Bedürfnisse abstimmen.\n",
    "\n",
    "Wenn die Aufgabe nicht gerade ist, Dinge zu erkennen, die im COCO-Datensatz enthalten sind (Personen, Tische, Handys, ...), müssen Sie dem Modell zwangsweise noch beibringen, wie die gewünschten Objekte aussehen.\n",
    "\n",
    "Hier haben Sie zwei Möglichkeiten:\n",
    "\n",
    "1. Sie trainieren ein Modell von Grund auf neu.\n",
    "2. Sie nehmen ein vortrainiertes Modell und trainieren es mit den eigenen Bildern weiter.\n",
    "\n",
    "Der zweite Schritt wird als **Fine-Tuning** bezeichnet und macht sich die Technik des **Transfer-Learnings** zu nutze.\n",
    "\n",
    "Beim Transfer-Learning nutzen wir die Tatsache aus, dass das vorherige Modell bereits viele allgemeine Merkmale erlernt hat, die auch für unsere spezifische Aufgabe relevant sein könnten. \n",
    "\n",
    "Der Vorteil von Transfer-Learning liegt darin, dass man von den bereits erworbenen Fähigkeiten des Modells profitieren und somit oft mit weniger Daten und Rechenzeit auskommen kann. Der vortrainierte Teil des Modells erkennt grundlegende Merkmale wie Kanten, Formen und Textur, während der neu trainierte Teil auf die spezifischen Objekte oder Muster in unseren eigenen Daten fein abgestimmt wird.\n",
    "\n",
    "**Transfer-Learning ist besonders nützlich in Situationen, in denen es schwierig oder kostspielig ist, einen umfangreichen eigenen Datensatz zu sammeln.** Es ermöglicht uns, auf den Schultern bereits existierender Modelle zu stehen und diese für unsere individuellen Anforderungen anzupassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07b8e7-64d1-4dfa-a6c4-987a5e524648",
   "metadata": {},
   "source": [
    "## 4.4. Das Training\n",
    "\n",
    "Wenn alles vorbereitet ist, kann das Modell mit einem einfachen Befehl trainiert werden:\n",
    "\n",
    "```Python\n",
    "results = model.train(data='config.yaml', epochs=10)\n",
    "```\n",
    "\n",
    "Dazu müssen der `.train()`-Funktion zwei Argumente übergeben werden: \n",
    "* die `config.yaml`-Datei\n",
    "* die **Anzahl der Trainingsepochen** - eine Epoche ist ein gesamter Durchlauf **aller** Daten durch das Modell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85795f3d-56ee-4b90-a902-e5043f7661e3",
   "metadata": {},
   "source": [
    "### 4.4.1. Die Trainingsfunktion\n",
    "\n",
    "Um das Modell zu trainieren müssen Sie diesem nur mitteilen, wo die Trainingsdaten zu finden sind. Dafür haben Sie vorher die Konfigurationsdatei erzeugt. \n",
    "\n",
    "```Python\n",
    "results = model.train(data=DATA_PATH,   # Zeigt auf den Ordner mit der Konfigurationsdatei\n",
    "                      epochs=EPOCHS,    # 1 Epoche = ein Durchlauf aller Daten durch das Modell\n",
    "                      name=NAME,        # Name der einzelnen Trainingsdurchläufe\n",
    "                      project=SAVE_DIR) # Name des Ordners in dem die Modellergebnisse gespeichert werden     \n",
    "```\n",
    "\n",
    "Beim Ausführen der nachfolgenden Codezelle werden Sie dazu aufgefordert die Anzahl der **Trainingsepochen** einzugeben. Wie lange das Modell trainieren soll ist immer problemabhängig, aber für ein leichtes Problem sollten 30-50 Epochen vollkommen ausreichend sein. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus, um das Modell zu trainieren. Verändern Sie gerne die Anzahl der Epochen. Beobachten Sie dabei die Ausgabe - während des Trainings werden verschiedene Metriken getrackt.\n",
    "</div>\n",
    "\n",
    "Grundsätzlich wollen wir, dass der **Modellfehler** (*loss*) im Verlauf des Trainings sinkt, und die **Genauigkeit** (*precision*) steigt. \n",
    "\n",
    "Beim Training des YOLO-Modells werden verschiedene Losses (**box_loss, cls_loss, dfl_loss**) und drei Precision-Metriken mitverfolgt (**P, mAP50, mAP50-95**). Was genau sich hinter den Metriken versteckt, können Sie in den nachfolgenden Details nachlesen.\n",
    "\n",
    "<details>\n",
    "<summary>Was bedeuten die Metriken?</summary>\n",
    "\n",
    "* **Box Loss (Bounding Box Loss):**\n",
    "Die Bounding Box Loss ist eine Kennzahl, die die Genauigkeit der Vorhersagen für begrenzende Kästen (BoundingBoxes) misst. Es bewertet, wie gut das Modell die Position und Größe eines erkannten Objekts vorhersagt, indem es die Differenz zwischen den vorhergesagten und den tatsächlichen begrenzenden Kastenwerten quantifiziert.\n",
    "\n",
    "* **Cls Loss (Classification Loss):**\n",
    "Die Klassifikationsverlustfunktion misst die Genauigkeit der vorhergesagten Klassen. In der Objekterkennung geht es darum, nicht nur die Position der Objekte zu lokalisieren (BoundingBox), sondern auch die Objekte selbst zu identifizieren. Die Klassifikationsverlustfunktion bewertet, wie gut das Modell die richtige Klasse für ein erkanntes Objekt vorhersagt.\n",
    "\n",
    "* **Dfl Loss (Directional Field Loss):**\n",
    "Der Directional Field Loss ist relevant für Aufgaben, bei denen die Orientierung oder Ausrichtung eines erkannten Objekts wichtig ist, wie zum Beispiel bei der Erkennung von Fahrzeugen. Diese Verlustfunktion bewertet die Genauigkeit der vorhergesagten Orientierung des erkannten Objekts im Vergleich zur tatsächlichen Orientierung.\n",
    "\n",
    "* **mAP50 (mean Average Precision at 50% IoU):**\n",
    "mAP steht für \"mean Average Precision\". Es ist eine Metrik, die die Qualität von Objekterkennungsmodellen bewertet. Der Wert 50% IoU (Intersection over Union) gibt an, wie gut die vorhergesagten BoundingBoxen mit den tatsächlichen BoundingBoxen übereinstimmen müssen, damit eine Vorhersage als korrekt betrachtet wird. mAP50 berechnet den durchschnittlichen Präzisionswert über verschiedene Klassen bei einem IoU-Schwellenwert von 50%.\n",
    "\n",
    "* **mAP95 (mean Average Precision at 95% IoU):**\n",
    "Ähnlich wie mAP50, jedoch mit einem höheren IoU-Schwellenwert von 95%. Dies bedeutet, dass die BoundingBoxen viel genauer mit den tatsächlichen BoundingBoxen übereinstimmen müssen, um als korrekt betrachtet zu werden. mAP95 gibt eine strengere Bewertung der Modellleistung.\n",
    "\n",
    "Außerdem werden Precision und Recall angezeigt:\n",
    "\n",
    "* **Precision / P (Genauigkeit):**\n",
    "Precision misst, wie viele der als positiv vorhergesagten Instanzen tatsächlich positiv sind. Es wird durch die Formel\n",
    "\n",
    "$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$\n",
    "\n",
    "berechnet. Hohe Precision bedeutet, dass die vorhergesagten positiven Instanzen tendenziell korrekt sind, jedoch sagt es nichts darüber aus, wie viele tatsächlich positive Instanzen das Modell verpasst hat (False Negatives).\n",
    "\n",
    "* **Recall / R (Sensitivität oder Trefferquote):**\n",
    "Recall misst, wie viele der tatsächlich positiven Instanzen vom Modell erkannt wurden. Es wird durch die Formel\n",
    "\n",
    "$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$\n",
    "\n",
    "berechnet. Hoher Recall bedeutet, dass das Modell eine hohe Anzahl der tatsächlich positiven Instanzen findet, aber es sagt nichts darüber aus, wie viele der vorhergesagten positiven Instanzen tatsächlich korrekt sind (True Positives im Verhältnis zu False Positives).\n",
    "\n",
    "Diese beiden Metriken sind oft im Konflikt zueinander. Eine Erhöhung der Precision kann zu einer Verringerung des Recalls führen und umgekehrt. Das F1-Score ist eine Metrik, die Precision und Recall kombiniert und häufig verwendet wird, um ein ausgewogenes Maß für die Leistung eines Modells zu erhalten. Der F1-Score wird durch die Formel\n",
    "\n",
    "$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "berechnet.\n",
    "\n",
    "</details>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "<b>Frage:</b> Wie entwickeln sich der Modellfehler und die Genauigkeit im Verlaufe des Trainings? Ist die Entwicklung wünschenswert?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe519d6-00a7-4461-92a8-fc310d00546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS, DATA_PATH, NAME, SAVE_DIR = set_training_config(PATH)\n",
    "\n",
    "results = model.train(data=DATA_PATH, \n",
    "                      epochs=EPOCHS, \n",
    "                      name=NAME,\n",
    "                      project=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e44393b-edec-4210-862e-d1f03c1d77d7",
   "metadata": {},
   "source": [
    "### 4.4.2. Was ist beim Training passiert?\n",
    "\n",
    "Nachdem das Modell trainiert wurde, wurde im Ordner `GRUPPENNAME/runs/` automatisch ein Ordner für den Trainingsdurchlauf angelegt. Die Inhalte dieses Ordners sehen folgendermaßen aus:\n",
    "\n",
    "![Image](../beispielbilder/grafiken/eval-detect.PNG)\n",
    "\n",
    "Im `weights`-Ordner befinden sich zwei Dateien: `best.pt` und `last.pt`:\n",
    "\n",
    "* `best.pt` ist der beste Zustand des Modells\n",
    "* `last.pt` ist der letzte Zustand des Modells\n",
    "\n",
    "Je nachdem ob Sie weitertrainieren möchten oder das Modell in die Produktionsumgebung überführen wollen, können Sie mit einer der beiden Dateien weiterarbeiten. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Frage:</b> Warum macht es im Hinblick auf Überanpassung Sinn, immer den besten Zustand des Modells zu speichern?</div>\n",
    "\n",
    "Außerdem enthält der Ordner noch eine Reihe an anderen Dateien:\n",
    "\n",
    "* Konfusionmatrizen\n",
    "* results.png\n",
    "* results.csv\n",
    "* Trainings- und Validationsbatches\n",
    "* ...\n",
    "\n",
    "Die Datei `results.png` zeigt beispielsweise eine Übersicht über verschiedene Metriken:\n",
    "\n",
    "![Image](../beispielbilder/grafiken/results.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Frage:</b> Nehmen Sie sich Zeit diese Dateien in Ruhe zu betrachten. Welche Informationen können Sie herauslesen?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0522f-c5de-43e4-979c-26b5280848c6",
   "metadata": {},
   "source": [
    "### 4.4.3. Das trainierte Modell verwenden \n",
    "\n",
    "Zu Beginn haben Sie ein Modell mithilfe des Befehls:\n",
    "\n",
    "```Python\n",
    "\n",
    "model = YOLO('pfad/zum/model.pt')\n",
    "```\n",
    "geladen.\n",
    "\n",
    "Dasselbe können Sie mit diesen Dateien auch machen!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Auftrag</b>: Führen Sie die nächste Codezelle aus, um das trainierte Modell zu laden.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d3d67-0f36-4887-a5fa-c2f7b6e5e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = get_trained_model(PATH) # extrahiert das zuletzt trainierte Modell,\n",
    "                                        # wenn ein bestimmter run ausgewählt werden soll, kann das mit dem 'run' Parameter gemacht werden,\n",
    "                                        # z.B. get_trained_model(PATH, run=2)\n",
    "trained_model = YOLO(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6dd6d-2b5a-4743-aa2a-05e176a93a35",
   "metadata": {},
   "source": [
    "Somit haben Sie in der Variable `trained_model` die beste Version des akutellen Trainingsdurchlaufs gespeichert. \n",
    "Dieses Modell können Sie jetzt genauso verwenden, wie Sie die Basis-Version des Modells zu Beginn verwendet haben:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>Auftrag</b>: Führen Sie die nächste Codezelle aus und testen Sie nun ob Ihr Modell die Gegenstände erkennt wenn Sie diese vor die Webcam halten. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a526e-ee6d-40c5-a5a1-b2d49241e89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_webcam(model=trained_model,\n",
    "                 device=0,\n",
    "                 verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550f55b5-4845-47cc-abf7-6596c928b108",
   "metadata": {},
   "source": [
    "## 4.5. Evaluieren des Modells\n",
    "\n",
    "Evaluieren Sie die Leistung des trainierten Modells und überprüfen, ob es bereits die gewünschten Ergebnisse erzielt. Falls nicht, sollten Sie mögliche Gründe analysieren und Ansatzpunkte identifizieren die zur Verbesserung des Modells beitragen könnten. Betrachten Sie insbesondere folgende Faktoren:\n",
    "\n",
    "**Trainingszeit:** Wurde das Modell ausreichend lange trainiert? Eine längere Trainingsdauer könnte zu einer besseren Leistung führen.\n",
    "\n",
    "**Datenqualität:** Wie ist die Qualität der Trainingsdaten? Sind sie ausreichend hochwertig und decken sie verschiedene Szenarien ab? Haben Sie ausreichend Daten?\n",
    "\n",
    "*Weitere Hyperparameter:* Die betrachten wir nicht direkt - das würde zu weit führen. Behalten Sie aber im Hinterkopf, dass auch hier Veränderungen vorgenommen werden könnten.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>Aufgabe:</b> Reflektieren Sie über potenzielle Gründe, warum das Modell in bestimmten Situationen nicht die erwarteten Ergebnisse liefert. Entwickeln Sie Strategien zur Lösung dieser Probleme und zur weiteren Verbesserung der Modellleistung.\n",
    "</div>\n",
    "\n",
    "<details>\n",
    "<summary>Tipps</summary>\n",
    "    <h3>Tips zur Verbesserung des trainierten YOLO-Objekterkennungsmodells:</h2>\n",
    "    <ul>\n",
    "        <li><strong>Visualisiert eure Vorhersagen:</strong> Nutzt Visualisierungen, um die Vorhersagen des Modells auf neuen Bildern zu überprüfen. Schaut euch an, wo das Modell Fehler macht, und versucht, Muster zu erkennen.</li>\n",
    "        <li><strong>Achtet auf fehlende Klassen:</strong> Kontrolliert, ob das Modell bestimmte Objektklassen nicht erkennt. Vielleicht müsst ihr mehr Trainingsdaten für diese Klassen sammeln oder die Gewichtung der Klassen anpassen.</li>\n",
    "        <li><strong>Überprüft die Klassenverteilung:</strong> Stellt sicher, dass die Anzahl der Trainingsbilder für jede Klasse ungefähr gleich ist. Falls nicht, kann das zu Ungleichgewichten führen.</li>\n",
    "        <li><strong>Passt die Trainingszeit an:</strong> Wenn nötig, erhöht die Anzahl der Trainingsepochen, um sicherzustellen, dass das Modell gut konvergiert und optimale Gewichtungen erreicht.</li>\n",
    "        <li><strong>Kontrolliert die Datenqualität:</strong> Schaut euch die Qualität der Trainingsdaten an. Stellt sicher, dass die Beschriftungen korrekt sind, minimiert Rauschen und sorgt für eine Vielfalt in den Daten.</li>\n",
    "        <li><strong>Berücksichtigt die Objektgrößen:</strong> Denkt darüber nach, ob das Modell für die Größe der erkannten Objekte optimal eingestellt ist. Das Hinzufügen von Daten mit unterschiedlichen Objektgrößen könnte hilfreich sein.</li>\n",
    "        <li><strong>Experimentiert mit Augmentierung:</strong> Probiert verschiedene Augmentierungstechniken aus, um die Datenvielfalt zu erhöhen und das Modell robuster gegenüber verschiedenen Bedingungen zu machen.</li>\n",
    "        <li><strong>Nutzt Transfer Learning:</strong> Falls möglich, startet das Training mit einem bereits auf großen Datensätzen trainierten YOLO-Modell und feintuned es auf eure spezifische Aufgabe.</li>\n",
    "        <li><strong>Achtet auf Overfitting:</strong> Kontrolliert, ob das Modell möglicherweise zu sehr auf die Trainingsdaten angepasst ist (Overfitting). Reguliert dies durch Anpassungen von Regularisierungs- und Dropout-Techniken.</li>\n",
    "        <li><strong>Beschriftungen überprüfen:</strong> Stellt sicher, dass die Beschriftungen (Annotationen) korrekt sind und jedes Objekt in den Bildern genau markiert ist. Fehlerhafte Beschriftungen können zu Verwirrungen und schlechter Leistung führen.</li>\n",
    "        <li><strong>Verschiedene Aufnahmeszenarien einbeziehen:</strong> Berücksichtigt verschiedene Beleuchtungsverhältnisse, Wetterbedingungen und Hintergründe, um sicherzustellen, dass das Modell in verschiedenen Umgebungen gut funktioniert.</li>\n",
    "        <li><strong>Objektvariationen erfassen:</strong> Achtet darauf, dass verschiedene Variationen der Objekte eurer Klassen in den Daten enthalten sind. Das Modell sollte in der Lage sein, Objekte unabhängig von ihrer Position, Orientierung oder Größe zu erkennen.</li>\n",
    "        <li><strong>Berücksichtigung von Schatten und Reflexionen:</strong> Wenn möglich, nehmt Bilder auf, die Schatten oder Reflexionen der Objekte enthalten. Dies stellt sicher, dass das Modell auch unter realen Bedingungen robust arbeitet.</li>\n",
    "        <li><strong>Mehr Daten sammeln:</strong> Je mehr Daten ihr habt, desto besser. Sammelt weitere Bilder, um die Vielfalt eurer Daten zu erhöhen und dem Modell mehr Beispiele für jede Klasse zu bieten.</li>\n",
    "        <li><strong>Korrektes Labeling von schwierigen Fällen:</strong> Achtet besonders auf schwierige Fälle, in denen Objekte teilweise verdeckt oder schwer zu erkennen sind. Stellt sicher, dass diese Fälle korrekt beschriftet sind, um das Modell in schwierigen Situationen zu verbessern.</li>\n",
    "        <li><strong>Bewegte Objekte erfassen:</strong> Wenn möglich, fangt auch Bilder von sich bewegenden Objekten ein. Dies ist besonders relevant, wenn eure Anwendung bewegte Objekte enthält.</li>\n",
    "        <li><strong>Verschiedene Blickwinkel einbeziehen:</strong> Variiert die Aufnahmeperspektiven, um sicherzustellen, dass das Modell Objekte aus verschiedenen Blickwinkeln erkennen kann.</li>\n",
    "        <li><strong>Kontinuierliche Verbesserung:</strong> Überprüft eure Daten regelmäßig und verbessert die Beschriftungen oder fügt neue Daten hinzu, um die Qualität im Laufe der Zeit zu steigern.</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505378e-7cdc-4640-b026-0f7b7cde845e",
   "metadata": {},
   "source": [
    "## 4.6. Re-Training\n",
    "\n",
    "Wenn nötig, trainieren Sie das Modell neu!\n",
    "\n",
    "**Training**\n",
    "<div class=\"alert alert-block alert-success\"> \n",
    "<b>Auftrag:</b> Führen Sie die nächste Codezelle aus, um das Modell neu zu trainieren.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01fd16-2d10-48d4-80c0-0fec1109da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = choose_model(task='DETECT')\n",
    "\n",
    "EPOCHS = int(input('Anzahl Epochen: '))\n",
    "DATA_PATH = os.path.join(PATH, 'config.yaml')\n",
    "NAME = f'detect-{GROUP}'\n",
    "SAVE_DIR = os.path.join(PATH,'runs')\n",
    "\n",
    "results = model.train(data=DATA_PATH, \n",
    "                      epochs=EPOCHS, \n",
    "                      name=NAME,\n",
    "                      project=SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397549-4d64-45b0-93ad-365530773f18",
   "metadata": {},
   "source": [
    "Testen Sie das neu trainierte Modell! Achten Sie darauf, das Modell aus dem richtigen Trainingsdurchlauf zu verwenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc3072-1c6f-4035-bd34-9e5ede5da2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model = os.path.join(PATH, 'runs', f'{NAME}', 'weights', 'best.pt')\n",
    "trained_model = YOLO(latest_model)\n",
    "\n",
    "inference_webcam(model=trained_model,\n",
    "                 device=0,\n",
    "                 verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
